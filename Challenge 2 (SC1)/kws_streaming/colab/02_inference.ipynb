{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "02_inference.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXYgXFeMgRep"
   },
   "source": [
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "NcIzzCADklYm",
    "colab": {}
   },
   "source": [
    "!git clone https://github.com/google-research/google-research.git"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ngihcW7ckrDI",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# sys.path.append('./google-research')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y55h79H3XKSt"
   },
   "source": [
    "# Examples of streaming and non streaming inference with TF/TFlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fathHzuEgx8_"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "yP5WBy5O8Za8",
    "colab": {}
   },
   "source": [
    "# TF streaming\n",
    "from kws_streaming.models import models\n",
    "from kws_streaming.models import utils\n",
    "from kws_streaming.layers.modes import Modes\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import logging\n",
    "from kws_streaming.models import model_flags\n",
    "from kws_streaming.models import model_params\n",
    "from kws_streaming.train import test\n",
    "from kws_streaming import data\n",
    "\n",
    "config = tf1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf1.Session(config=config)\n",
    "\n",
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal\n",
    "import tensorflow_io as tfio\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "tf1.reset_default_graph()\n",
    "sess = tf1.Session()\n",
    "tf1.keras.backend.set_session(sess)\n",
    "tf1.keras.backend.set_learning_phase(0)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylPGCTPLh41F"
   },
   "source": [
    "## Load wav file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "b8Bvq7XacsOu",
    "colab": {}
   },
   "source": [
    "def waveread_as_pcm16(filename):\n",
    "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
    "  with tf.io.gfile.GFile(filename, 'rb') as file_handle:\n",
    "    samplerate, wave_data = wav.read(file_handle)\n",
    "  # Read in wav file.\n",
    "  return wave_data, samplerate\n",
    "\n",
    "def wavread_as_float(filename, target_sample_rate=16000):\n",
    "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
    "  wave_data, samplerate = waveread_as_pcm16(filename)\n",
    "  desired_length = int(\n",
    "      round(float(len(wave_data)) / samplerate * target_sample_rate))\n",
    "  wave_data = scipy.signal.resample(wave_data, desired_length)\n",
    "\n",
    "  # Normalize short ints to floats in range [-1..1).\n",
    "  data = np.array(wave_data, np.float32) / 32768.0\n",
    "  return data, target_sample_rate"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e6MDIFztU7Lp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# set PATH to data sets (for example to speech commands V2):\n",
    "# it can be downloaded from\n",
    "# https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
    "# if you run 00_check-data.ipynb then data2 should be located in the current folder\n",
    "current_dir = 'C:/Users/alien/Documents/PyCharm-Projects/keyword-transformer/'\n",
    "DATA_PATH = os.path.join(current_dir, \"data2/\")"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "TYj0JGeHhtqc",
    "colab": {}
   },
   "source": [
    "samplerate = 16000\n",
    "# read audio file\n",
    "audio, _ = tf.audio.decode_wav(tf.io.read_file('C:/Users/alien/Documents/PyCharm-Projects/keyword-transformer/s1_test_release/s1_test_0000.wav'))\n",
    "waveform = tf.squeeze(audio, axis=-1)\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "U7VYKfWoh_3-",
    "colab": {}
   },
   "source": [
    "# for simple test instead of reading wav - just generate cos\n",
    "# samplerate = 16000\n",
    "# wav_data = np.cos(2.0*np.pi*8.0*np.arange(samplerate)/samplerate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "jNiuJTvXiF1J",
    "colab": {}
   },
   "source": [
    "#assert samplerate == 16000\n",
    "#sound.Play(wav_data, samplerate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "r2yeKkLsiRWJ",
    "colab": {}
   },
   "source": [
    "plt.plot(waveform.numpy())"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-4b7dba5e57b5>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwaveform\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_wbAZ3vhQh1"
   },
   "source": [
    "## Prepare batched model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SYl2VSAhU7L_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Set path to model weights and model parameters models trained on data set V2 can be downloaded from\n",
    "# https://storage.googleapis.com/kws_models/models2.zip\n",
    "# or from https://storage.googleapis.com/kws_models/models2_30k.zip\n",
    "MODEL_URL = \"https://storage.googleapis.com/kws_models/models2_30k.zip\"\n",
    "# base_name = os.path.basename(MODEL_URL)\n",
    "MODELS_PATH = 'C:/Users/alien/Documents/PyCharm-Projects/keyword-transformer/models_data_v2_12_labels/97.65625/'\n",
    "# base_name"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mH-fyuESU7MD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# it can take some time to download 2.3GB. After unpacking total size is 5.4GB\n",
    "# arch_file_name = os.path.join(MODELS_PATH, base_name)\n",
    "# if not os.path.isfile(arch_file_name):\n",
    "#   # download data\n",
    "#   if sys.version_info >= (2, 5):\n",
    "#     file_path = urllib.request.urlretrieve(MODEL_URL, filename=arch_file_name)[0]\n",
    "#   else:\n",
    "#     file_path = urllib.urlretrieve(MODEL_URL, filename=arch_file_name)[0]\n",
    "#\n",
    "#   # unpack it\n",
    "#   file_name, file_extension = os.path.splitext(base_name)\n",
    "#   with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(MODELS_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XDZTRXCxU7MF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# train_dir = os.path.join(MODELS_PATH, file_name, 'svdf')\n",
    "# urllib.request.urlretrieve(MODEL_URL, filename='C:/Users/alien/Documents/PyCharm-Projects/keyword-transformer/models2.zip')[0]"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "e2f-1Ioqbn4G",
    "colab": {}
   },
   "source": [
    "# load command line command flags which were use for model creation/training\n",
    "from argparse import Namespace\n",
    "with open(os.path.join(MODELS_PATH, 'flags.txt'), 'r') as fd:\n",
    "  flags_txt = fd.read()\n",
    "flags = eval(flags_txt)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I2x6dAhgU7ML",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# below is another way of reading flags - through json\n",
    "# with tf.compat.v1.gfile.Open(os.path.join(train_dir, 'flags.json'), 'r') as fd:\n",
    "#   flags_json = json.load(fd)\n",
    "\n",
    "# class DictStruct(object):\n",
    "#   def __init__(self, **entries):\n",
    "#     self.__dict__.update(entries)\n",
    "\n",
    "# flags = DictStruct(**flags_json)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PSfkGIgAU7MO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "flags.data_dir = DATA_PATH"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U1al1r1PU7MR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# pad input audio with zeros, so that audio len = flags.desired_samples\n",
    "zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
    "# Concatenate audio with padding so that all audio clips will be of the same length\n",
    "waveform = tf.cast(waveform, tf.float32)\n",
    "equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "input_data = tf.expand_dims(equal_length, 0)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gkWTgtOWU7MT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# prepare mapping of index to word\n",
    "audio_processor = data.input_data.AudioProcessor(flags)\n",
    "index_to_label = {}\n",
    "# labels used for training\n",
    "for word in audio_processor.word_to_index.keys():\n",
    "  if audio_processor.word_to_index[word] == data.input_data.SILENCE_INDEX:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = data.input_data.SILENCE_LABEL\n",
    "  elif audio_processor.word_to_index[word] == data.input_data.UNKNOWN_WORD_INDEX:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = data.input_data.UNKNOWN_WORD_LABEL\n",
    "  else:\n",
    "    index_to_label[audio_processor.word_to_index[word]] = word\n",
    "\n",
    "# training labels\n",
    "index_to_label"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{2: 'bird',\n 3: 'eight',\n 4: 'falcon',\n 5: 'five',\n 6: 'four',\n 7: 'nine',\n 8: 'one',\n 9: 'seven',\n 10: 'six',\n 11: 'snake',\n 12: 'three',\n 13: 'two',\n 14: 'zero',\n 0: '_silence_'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "WlGR0oOxnizg",
    "colab": {}
   },
   "source": [
    "# we can create a dummy model\n",
    "# flags = model_params.HOTWORD_MODEL_PARAMS['gru']\n",
    "# flags = model_flags.update_flags(flags)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "wsGDG4A0cIMO",
    "colab": {}
   },
   "source": [
    "# create model with flag's parameters\n",
    "model_non_stream_batch = models.MODELS[flags.model_name](flags)\n",
    "\n",
    "# load model's weights\n",
    "weights_name = 'best_weights'\n",
    "model_non_stream_batch.load_weights(os.path.join(MODELS_PATH, weights_name))"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:390: UserWarning: Default value of `approximate` is changed from `True` to `False`\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2cee07767f0>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "dsWLekwbkdTo",
    "colab": {}
   },
   "source": [
    "#model_non_stream_batch.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "QVhESthmMl0X",
    "colab": {}
   },
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model_non_stream_batch,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True, to_file='model.png')"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIr1DWLisMu9"
   },
   "source": [
    "## Run inference with TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "456ynjRxmdVc"
   },
   "source": [
    "### TF Run non streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-vJpOCJClDK5",
    "colab": {}
   },
   "source": [
    "# convert model to inference mode with batch one\n",
    "inference_batch_size = 1\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "flags.batch_size = inference_batch_size  # set batch size\n",
    "model_non_stream_batch.summary()\n",
    "# model_non_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.NON_STREAM_INFERENCE)\n",
    "# model_non_stream.summary()"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(128, 16000)]            0         \n",
      "_________________________________________________________________\n",
      "speech_features (SpeechFeatu (128, 98, 40)             0         \n",
      "_________________________________________________________________\n",
      "kws_transformer (KWSTransfor (128, 192)                5358528   \n",
      "_________________________________________________________________\n",
      "sequential_12 (Sequential)   multiple                  2895      \n",
      "=================================================================\n",
      "Total params: 5,361,423\n",
      "Trainable params: 5,361,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "O1gOGQjWMufh",
    "colab": {}
   },
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model_non_stream,\n",
    "#     show_shapes=True,\n",
    "#     show_layer_names=True,\n",
    "#     expand_nested=True, to_file='model.png')"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_non_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-22-a9964cb1c038>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m tf.keras.utils.plot_model(\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mmodel_non_stream\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[0mshow_shapes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mshow_layer_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     expand_nested=True, to_file='model.png')\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model_non_stream' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "nPUfT4a4lxIj",
    "colab": {}
   },
   "source": [
    "predictions = model_non_stream_batch.predict(input_data, steps=128)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\n\t [[{{node speech_features/data_frame/frame/Reshape}}]]\n\t [[sequential_12/dense_73/BiasAdd/_333]]\n  (1) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\n\t [[{{node speech_features/data_frame/frame/Reshape}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-29-1eaa78980995>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpredictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_non_stream_batch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msteps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mpredicted_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m    980\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    981\u001B[0m     \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_select_training_loop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 982\u001B[1;33m     return func.predict(\n\u001B[0m\u001B[0;32m    983\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    984\u001B[0m         \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    704\u001B[0m     x, _, _ = model._standardize_user_data(\n\u001B[0;32m    705\u001B[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001B[1;32m--> 706\u001B[1;33m     return predict_loop(\n\u001B[0m\u001B[0;32m    707\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    708\u001B[0m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001B[0m in \u001B[0;36mmodel_iteration\u001B[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001B[0m\n\u001B[0;32m    292\u001B[0m           \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m             \u001B[0mactual_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mins\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 294\u001B[1;33m           \u001B[0mbatch_outs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mactual_inputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    295\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    296\u001B[0m           \u001B[1;32mif\u001B[0m \u001B[0mis_dataset\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   3954\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_callable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeed_arrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_symbols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msymbol_vals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3955\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3956\u001B[1;33m     fetched = self._callable_fn(*array_vals,\n\u001B[0m\u001B[0;32m   3957\u001B[0m                                 run_metadata=self.run_metadata)\n\u001B[0;32m   3958\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1479\u001B[0m         \u001B[0mrun_metadata_ptr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_NewBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1480\u001B[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[0m\u001B[0;32m   1481\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1482\u001B[0m                                                run_metadata_ptr)\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: 2 root error(s) found.\n  (0) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\n\t [[{{node speech_features/data_frame/frame/Reshape}}]]\n\t [[sequential_12/dense_73/BiasAdd/_333]]\n  (1) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\n\t [[{{node speech_features/data_frame/frame/Reshape}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "63sisD1hl7jz",
    "colab": {}
   },
   "source": [
    "predicted_labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "rBhLA1OZmQxj",
    "colab": {}
   },
   "source": [
    "index_to_label[predicted_labels[0]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVFoVdYSpnL_"
   },
   "source": [
    "### TF Run streaming inference with internal state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "cgcpcrASquAY",
    "colab": {}
   },
   "source": [
    "# convert model to streaming mode\n",
    "flags.batch_size = inference_batch_size  # set batch size\n",
    "\n",
    "model_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.STREAM_INTERNAL_STATE_INFERENCE)\n",
    "#model_stream.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "BNtgTOBCM06v",
    "colab": {}
   },
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model_stream,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "7NOG8wrYpnnq",
    "colab": {}
   },
   "source": [
    "# run streaming inference\n",
    "start = 0\n",
    "end = flags.window_stride_samples\n",
    "while end <= input_data.shape[1]:\n",
    "  stream_update = input_data[:, start:end]\n",
    "\n",
    "  # get new frame from stream of data\n",
    "  stream_output_prediction = model_stream.predict(stream_update)\n",
    "  stream_output_arg = np.argmax(stream_output_prediction)\n",
    "\n",
    "  # update indexes of streamed updates\n",
    "  start = end\n",
    "  end = start + flags.window_stride_samples\n",
    "\n",
    "stream_output_arg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "S-xeXPhAqC20",
    "colab": {}
   },
   "source": [
    "index_to_label[stream_output_arg]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5WYgOtSqrQb"
   },
   "source": [
    "### TF Run streaming inference with external state"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "2hTLEY1qq_ig",
    "colab": {}
   },
   "source": [
    "# convert model to streaming mode\n",
    "flags.batch_size = inference_batch_size  # set batch size\n",
    "\n",
    "model_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.STREAM_EXTERNAL_STATE_INFERENCE)\n",
    "#model_stream.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "AyeABeg9Mbf6",
    "colab": {}
   },
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model_stream,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "RISdLTnmqrcA",
    "colab": {}
   },
   "source": [
    "\n",
    "inputs = []\n",
    "for s in range(len(model_stream.inputs)):\n",
    "  inputs.append(np.zeros(model_stream.inputs[s].shape, dtype=np.float32))\n",
    "\n",
    "reset_state = True\n",
    "\n",
    "if reset_state:\n",
    "  for s in range(len(model_stream.inputs)):\n",
    "    inputs[s] = np.zeros(model_stream.inputs[s].shape, dtype=np.float32)\n",
    "\n",
    "start = 0\n",
    "end = flags.window_stride_samples\n",
    "while end <= input_data.shape[1]:\n",
    "  # get new frame from stream of data\n",
    "  stream_update = input_data[:, start:end]\n",
    "\n",
    "  # update indexes of streamed updates\n",
    "  start = end\n",
    "  end = start + flags.window_stride_samples\n",
    "\n",
    "  # set input audio data (by default input data at index 0)\n",
    "  inputs[0] = stream_update\n",
    "\n",
    "  # run inference\n",
    "  outputs = model_stream.predict(inputs)\n",
    "\n",
    "  # get output states and set it back to input states\n",
    "  # which will be fed in the next inference cycle\n",
    "  for s in range(1, len(model_stream.inputs)):\n",
    "    inputs[s] = outputs[s]\n",
    "\n",
    "  stream_output_arg = np.argmax(outputs[0])\n",
    "stream_output_arg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "u6p1xubwrYyo",
    "colab": {}
   },
   "source": [
    "index_to_label[stream_output_arg]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAJs5dBXsYCa"
   },
   "source": [
    "## Run inference with TFlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5qmO5KrU7NP"
   },
   "source": [
    "### Run non streaming inference with TFLite"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "88bclN4rtu-5",
    "colab": {}
   },
   "source": [
    "path = os.path.join('C:/Users/alien/Documents/PyCharm-Projects/keyword-transformer/models_data_v2_12_labels/96.5625/tflite_non_stream')\n",
    "tflite_model_name = 'non_stream.tflite'\n",
    "\n",
    "# tflite_non_streaming_model = utils.model_to_tflite(sess, model_non_stream, flags, Modes.NON_STREAM_INFERENCE)\n",
    "\n",
    "# prepare TFLite interpreter\n",
    "with open(os.path.join(path, tflite_model_name), 'rb') as f:\n",
    "  model_content = f.read()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=model_content)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "inputs = []\n",
    "for s in range(len(input_details)):\n",
    "  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "3J2n7VB5JxV6",
    "colab": {}
   },
   "source": [
    "# pad input audio with zeros, so that audio len = flags.desired_samples\n",
    "zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
    "# Concatenate audio with padding so that all audio clips will be of the same length\n",
    "waveform = tf.cast(waveform, tf.float32)\n",
    "equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "input_data = tf.expand_dims(equal_length, 0)"
   ],
   "execution_count": 53,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-53-357e7737b932>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mpadded_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m16000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mpadded_input\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[0minput_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    399\u001B[0m         \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnp_config\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    400\u001B[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001B[1;32m--> 401\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    402\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    403\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "TXqHxLcVregL",
    "colab": {}
   },
   "source": [
    "# set input audio data (by default input data at index 0)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.float32))\n",
    "\n",
    "# run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# get output: classification\n",
    "out_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "out_tflite_argmax = np.argmax(out_tflite)\n",
    "\n",
    "out_tflite_argmax"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "\n        'Tensor' object has no attribute 'astype'.\n        If you are looking for numpy-related methods, please run the following:\n        import tensorflow.python.ops.numpy_ops.np_config\n        np_config.enable_numpy_behavior()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-55-a1f77b3ae9f0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# set input audio data (by default input data at index 0)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0minterpreter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_details\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'index'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# run inference\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0minterpreter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minvoke\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    394\u001B[0m                 \"tolist\", \"data\"}:\n\u001B[0;32m    395\u001B[0m       \u001B[1;31m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 396\u001B[1;33m       raise AttributeError(\"\"\"\n\u001B[0m\u001B[0;32m    397\u001B[0m         \u001B[1;34m'{}'\u001B[0m \u001B[0mobject\u001B[0m \u001B[0mhas\u001B[0m \u001B[0mno\u001B[0m \u001B[0mattribute\u001B[0m \u001B[1;34m'{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    398\u001B[0m         \u001B[0mIf\u001B[0m \u001B[0myou\u001B[0m \u001B[0mare\u001B[0m \u001B[0mlooking\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mrelated\u001B[0m \u001B[0mmethods\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mplease\u001B[0m \u001B[0mrun\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mfollowing\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: \n        'Tensor' object has no attribute 'astype'.\n        If you are looking for numpy-related methods, please run the following:\n        import tensorflow.python.ops.numpy_ops.np_config\n        np_config.enable_numpy_behavior()"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "KbIB3zaiKEru",
    "colab": {}
   },
   "source": [
    "print(out_tflite)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1vOhJRnCU7Nf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "index_to_label[out_tflite_argmax]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNaUWgivuatL"
   },
   "source": [
    "### Run streaming inference with TFLite"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "csQWZo4BuqEB",
    "colab": {}
   },
   "source": [
    "# path = os.path.join(train_dir, 'tflite_stream_state_external')\n",
    "# tflite_model_name = 'stream_state_external.tflite'\n",
    "\n",
    "tflite_streaming_model = utils.model_to_tflite(sess, model_non_stream, flags, Modes.STREAM_EXTERNAL_STATE_INFERENCE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "a4wAZqYouyob",
    "colab": {}
   },
   "source": [
    "# with tf.io.gfile.Open(os.path.join(path, tflite_model_name), 'rb') as f:\n",
    "#   model_content = f.read()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_streaming_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "inputs = []\n",
    "for s in range(len(input_details)):\n",
    "  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "03QCq1nfVUWW",
    "colab": {}
   },
   "source": [
    "input_details[0]['shape']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "WKudF1Zyud2-",
    "colab": {}
   },
   "source": [
    "reset_state = True\n",
    "\n",
    "# before processing new test sequence we can reset model state\n",
    "# if we reset model state then it is not real streaming mode\n",
    "if reset_state:\n",
    "  for s in range(len(input_details)):\n",
    "    print(input_details[s]['shape'])\n",
    "    inputs[s] = np.zeros(input_details[s]['shape'], dtype=np.float32)\n",
    "\n",
    "start = 0\n",
    "end = flags.window_stride_samples\n",
    "while end <= input_data.shape[1]:\n",
    "  stream_update = input_data[:, start:end]\n",
    "  stream_update = stream_update.astype(np.float32)\n",
    "\n",
    "  # update indexes of streamed updates\n",
    "  start = end\n",
    "  end = start + flags.window_stride_samples\n",
    "\n",
    "  # set input audio data (by default input data at index 0)\n",
    "  interpreter.set_tensor(input_details[0]['index'], stream_update)\n",
    "\n",
    "  # set input states (index 1...)\n",
    "  for s in range(1, len(input_details)):\n",
    "    interpreter.set_tensor(input_details[s]['index'], inputs[s])\n",
    "\n",
    "  # run inference\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # get output: classification\n",
    "  out_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "  #print(start / 16000.0, np.argmax(out_tflite), np.max(out_tflite))\n",
    "\n",
    "  # get output states and set it back to input states\n",
    "  # which will be fed in the next inference cycle\n",
    "  for s in range(1, len(input_details)):\n",
    "    # The function `get_tensor()` returns a copy of the tensor data.\n",
    "    # Use `tensor()` in order to get a pointer to the tensor.\n",
    "    inputs[s] = interpreter.get_tensor(output_details[s]['index'])\n",
    "\n",
    "  out_tflite_argmax = np.argmax(out_tflite)\n",
    "out_tflite_argmax"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "yWy_BiepFFSX",
    "colab": {}
   },
   "source": [
    "print(out_tflite)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "QSa7AX1GvReF",
    "colab": {}
   },
   "source": [
    "index_to_label[out_tflite_argmax]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WVQZXLyDU7N3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}