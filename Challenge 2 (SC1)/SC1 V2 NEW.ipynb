{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SC1 V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47xnB0HBDS6W"
   },
   "source": [
    "# Install & import required packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ty1DE7LJDCt6",
    "outputId": "9c5439f2-5f27-4070-8ff3-3dc7233e8a56"
   },
   "source": [
    "# !pip install tensorflow_io tensorflow_addons pydub"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "TS68NZ1RC5xB",
    "outputId": "3610446b-8bba-458b-9115-80c2044d315d"
   },
   "source": [
    "import os, sys, shutil\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from IPython import display\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow.image import ResizeMethod\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing, Normalization\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(f'Running on Python {sys.version}, Tensorflow {tf.__version__}.')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "c:\\program files\\python38\\lib\\site-packages\\tensorflow_io\\core\\python\\ops\\__init__.py:103: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so']\n",
      "caused by: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so not found']\n",
      "  warnings.warn(\"file system plugins are not loaded: {}\".format(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Python 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)], Tensorflow 2.4.0-rc1.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frV-53r4DYU3"
   },
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YpeF1aZJDb64",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cdf0e22e-49b7-45be-f012-bca5c3c4e9cd"
   },
   "source": [
    "# # train\n",
    "# !gdown --id 19MpqPlGu23hfsF-kKM72VCU2AhC7fQ2a\n",
    "# # !gdown --id 1UjmCtdOSP-HGPVlMKCLRzTkIQJ65CIcs # extra\n",
    "#\n",
    "# # test\n",
    "# !gdown --id 1ix2wilO57zpEd-TcqNpUUQxRo0jzs4PH"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPKPFWeSFbaw"
   },
   "source": [
    "# Extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lpxw4V8VFeDF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aefd553a-7577-4df6-e0e8-02c8812dd5ab"
   },
   "source": [
    "# !unzip s1_release.zip\n",
    "# # !unzip s1_hidden.zip\n",
    "# !unzip s1_test_release.zip\n",
    "#\n",
    "# # for dir in os.listdir(\"s1_hidden\"):\n",
    "# #   for filename in os.listdir(f\"s1_hidden/{dir}\"):\n",
    "# #     if not os.path.exists(f\"s1_release/{dir}/{filename}\"):\n",
    "# #       shutil.copy(f\"s1_hidden/{dir}/{filename}\", f\"s1_release/{dir}\")"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1ocB373ThfQ"
   },
   "source": [
    "# Normalise average audio amplitude"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "IX_i2aehTmeg",
    "outputId": "017708c6-7511-47e1-d73f-601696d154d9"
   },
   "source": [
    "# def match_target_amplitude(sound, target_dBFS):\n",
    "#     change_in_dBFS = target_dBFS - sound.dBFS\n",
    "#     return sound.apply_gain(change_in_dBFS)\n",
    "#\n",
    "# for path in glob(\"**/*.wav\", recursive=True):\n",
    "#   split_path = path.split(\"/\")\n",
    "#   if split_path[0].endswith(\"normalized\"):\n",
    "#     continue\n",
    "#\n",
    "#   normalized_sound_dir = split_path[0] + \"_normalized\"\n",
    "#   if not os.path.exists(normalized_sound_dir):\n",
    "#     os.mkdir(normalized_sound_dir)\n",
    "#\n",
    "#   # create subdirectories if file has label\n",
    "#   if not split_path[1].endswith(\".wav\"):\n",
    "#     normalized_sound_label_subdir = f\"{normalized_sound_dir}/{split_path[1]}\"\n",
    "#     if not os.path.exists(normalized_sound_label_subdir):\n",
    "#       os.mkdir(normalized_sound_label_subdir)\n",
    "#\n",
    "#   sound = AudioSegment.from_file(path, \"wav\")\n",
    "#   normalized_sound = match_target_amplitude(sound, -20.0)\n",
    "#\n",
    "#   normalized_sound_path = split_path[0] + \"_normalized/\" + '/'.join(split_path[1:])\n",
    "#   normalized_sound_path = normalized_sound_path[:-4] + \"_normalized\" + normalized_sound_path[-4:]\n",
    "#   normalized_sound.export(normalized_sound_path, format=\"wav\")\n",
    "#\n",
    "# sample_rate = 16000\n",
    "#\n",
    "# # s1_release/falcon/train_falcon_0276.wav is quieter than most other files\n",
    "# # can't really hear the difference between displayed waveforms but I swear it's there\n",
    "# waveform = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file('s1_release/falcon/train_falcon_0276.wav'), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "# display.display(display.Audio(waveform, rate=sample_rate))\n",
    "#\n",
    "# waveform_normalized = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file('s1_release_normalized/falcon/train_falcon_0276_normalized.wav'), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "# display.display(display.Audio(waveform_normalized, rate=sample_rate))\n",
    "#\n",
    "# print(waveform)\n",
    "# print(waveform_normalized)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPge5GXRTnD7"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlKYZ35FC5xE",
    "outputId": "01893bf0-61ed-42f3-a192-4c3644ff5605"
   },
   "source": [
    "\n",
    "# TODO BEFORE SUBMITTING CODE:\n",
    "# remove val as test codes from data loading, see if need remove test files folder\n",
    "# uncomment test_preprocess func and codes in data loading\n",
    "\n",
    "# Data loading\n",
    "seed = 69\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "sample_rate = 16000\n",
    "\n",
    "# load normalized train set\n",
    "data_dir = pathlib.Path('s1_release_normalized')\n",
    "labels = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "num_labels = len(labels)\n",
    "print('Commands:', labels)\n",
    "\n",
    "# Original\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:', len(tf.io.gfile.listdir(str(data_dir/labels[0]))))\n",
    "print('Example file tensor:', filenames[0])\n",
    "\n",
    "train_files = filenames[:round(num_samples*0.8)]  # first 80%\n",
    "val_files = filenames[round(num_samples*0.8):]  # last 20%\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "\n",
    "# load given test set\n",
    "data_dir = pathlib.Path('s1_test_release_normalized')\n",
    "test_files = tf.io.gfile.glob(str(data_dir) + '/*')  # provided\n",
    "print('Test set size', len(test_files))\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['bird' 'eight' 'falcon' 'five' 'four' 'nine' 'one' 'seven' 'six' 'snake'\n",
      " 'three' 'two' 'zero']\n",
      "Number of train examples: 3250\n",
      "Number of train examples per label: 250\n",
      "Val set size 649\n",
      "Test set size 649\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "easpRSRSC5xF",
    "outputId": "7c07593b-c997-42fe-eb65-fb92615af557"
   },
   "source": [
    "# Preprocessing\n",
    "# Read file into AudioIOTensor, need to specify dtype as running it inside a tf.data function without eager execution CHECKED\n",
    "waveform = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file('s1_release/eight/train_eight_3153.wav'), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "print(waveform)\n",
    "display.display(display.Audio(waveform, rate=sample_rate))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.numpy())"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "unable to open file: libtensorflow_io.so, from paths: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so not found']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-9c4fe03f8803>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Preprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m# Read file into AudioIOTensor, need to specify dtype as running it inside a tf.data function without eager execution CHECKED\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mwaveform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtfio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maudio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode_wav\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m's1_release/eight/train_eight_3153.wav'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m32768.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwaveform\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAudio\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwaveform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrate\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msample_rate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow_io\\core\\python\\ops\\audio_ops.py\u001B[0m in \u001B[0;36mdecode_wav\u001B[1;34m(input, shape, dtype, name)\u001B[0m\n\u001B[0;32m    443\u001B[0m         \u001B[0mdtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    444\u001B[0m     ), \"dtype (tf.uint8/tf.int16/tf.int32/tf.float32) must be provided\"\n\u001B[1;32m--> 445\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mcore_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio_audio_decode_wav\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    446\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow_io\\core\\python\\ops\\__init__.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, attrb)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__dir__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow_io\\core\\python\\ops\\__init__.py\u001B[0m in \u001B[0;36m_load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     82\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mod\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mod\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_load_library\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_library\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow_io\\core\\python\\ops\\__init__.py\u001B[0m in \u001B[0;36m_load_library\u001B[1;34m(filename, lib)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNotFoundError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[0merrs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     raise NotImplementedError(\n\u001B[0m\u001B[0;32m     70\u001B[0m         \u001B[1;34m\"unable to open file: \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;33m+\u001B[0m \u001B[1;34m\"{}, from paths: {}\\ncaused by: {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilenames\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: unable to open file: libtensorflow_io.so, from paths: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so']\ncaused by: ['c:\\\\program files\\\\python38\\\\lib\\\\site-packages\\\\tensorflow_io\\\\core\\\\python\\\\ops\\\\libtensorflow_io.so not found']"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "lHQzeJBPC5xF",
    "outputId": "2d35c219-acb1-401d-f780-1b2d5dbdbcf9"
   },
   "source": [
    "# zero pad to 16000  CHECKED\n",
    "zero_padding = tf.zeros([sample_rate] - tf.shape(waveform), dtype=tf.float32)\n",
    "waveform = tf.cast(waveform, tf.float32)\n",
    "padded_audio_tensor = tf.concat([waveform, zero_padding], 0)\n",
    "display.display(display.Audio(padded_audio_tensor, rate=sample_rate))\n",
    "plt.figure()\n",
    "plt.plot(padded_audio_tensor.numpy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Hjhy60xUC5xG",
    "outputId": "4c9ae348-8aa1-49ed-bf91-f7551621d147"
   },
   "source": [
    "# spectrogram CHECKED\n",
    "spectrogram = tfio.audio.spectrogram(padded_audio_tensor, nfft=1024, window=1001, stride=31)\n",
    "spectrogram = tf.abs(spectrogram)\n",
    "plt.figure()\n",
    "plt.imshow(spectrogram.numpy())\n",
    "spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "spectrogram = tf.squeeze(tf.stack([spectrogram, spectrogram, spectrogram], axis=-1))\n",
    "print(spectrogram)\n",
    "print(f'spectrogram shape: {spectrogram.shape}')\n",
    "\n",
    "tf.reduce_all(tf.equal(spectrogram, spectrogram))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qr6H3KGPC5xG"
   },
   "source": [
    "# NOT CHECKED\n",
    "# # Trim noise (TODO: try with different gate, default 0.1)\n",
    "# position = tfio.audio.trim(audio_tensor, axis=0, epsilon=0.2)  # epsilon = noise gate (how loud between 0 to 1)\n",
    "# print(position)\n",
    "# start = position[0]\n",
    "# stop = position[1]\n",
    "# print(start, stop)\n",
    "# trimmed_audio_tensor = audio_tensor[start:stop]\n",
    "# display.display(display.Audio(trimmed_audio_tensor, rate=sample_rate))\n",
    "# plt.figure()\n",
    "# plt.plot(trimmed_audio_tensor.numpy())\n",
    "#\n",
    "# # Fade in fade out (TODO: to try)\n",
    "# faded_audio_tensor = tfio.audio.fade(trimmed_audio_tensor, fade_in=1000, fade_out=1000, mode=\"logarithmic\")\n",
    "# display.display(display.Audio(faded_audio_tensor, rate=sample_rate))\n",
    "# plt.figure()\n",
    "# plt.plot(faded_audio_tensor.numpy())\n",
    "#\n",
    "# # zero pad to 16000\n",
    "# zero_padding = tf.zeros([sample_rate] - tf.shape(faded_audio_tensor), dtype=tf.float32)\n",
    "# faded_audio_tensor = tf.cast(faded_audio_tensor, tf.float32)\n",
    "# padded_audio_tensor = tf.concat([faded_audio_tensor, zero_padding], 0)\n",
    "# display.display(display.Audio(padded_audio_tensor, rate=sample_rate))\n",
    "# plt.figure()\n",
    "# plt.plot(padded_audio_tensor.numpy())\n",
    "#\n",
    "# # Convert to spectrogram and merge into 3 channels\n",
    "# # first line is from paper, 2nd is halved\n",
    "# # stft_params = [{'nfft': 1024, 'window': 800, 'stride': 320}, {'nfft': 2048, 'window': 1600, 'stride': 800}, {'nfft': 4096, 'window': 3200, 'stride': 1600}]  # stride = hop size\n",
    "# stft_params = [{'nfft': 512, 'window': 400, 'stride': 160}, {'nfft': 1024, 'window': 800, 'stride': 400}, {'nfft': 2048, 'window': 1600, 'stride': 800}]  # stride = hop size\n",
    "# spectrogram = tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride'])\n",
    "# spectrogram_1 = tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride'])\n",
    "# spectrogram_2 = tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[1]['nfft'], window=stft_params[1]['window'], stride=stft_params[1]['stride'])\n",
    "# spectrogram_3 = tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[2]['nfft'], window=stft_params[2]['window'], stride=stft_params[2]['stride'])\n",
    "# print(f'spectrogram shape: {spectrogram.shape}')\n",
    "# plt.figure()\n",
    "# plt.imshow(spectrogram.numpy())\n",
    "#\n",
    "# # Convert to mel-spectrogram\n",
    "# mel_spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=250, fmin=20, fmax=8000)\n",
    "# plt.figure()\n",
    "# plt.imshow(mel_spectrogram.numpy())\n",
    "#\n",
    "# # Convert to db scale mel-spectrogram (TODO: to try)\n",
    "# dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "# plt.figure()\n",
    "# plt.imshow(dbscale_mel_spectrogram.numpy())\n",
    "#\n",
    "# # Convert to log-mel spectrogram\n",
    "# log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "# print(f'log spectrogram shape: {log_mel_spectrogram.shape}')\n",
    "# plt.figure()\n",
    "# plt.imshow(log_mel_spectrogram.numpy())\n",
    "#\n",
    "# # Freq masking\n",
    "# freq_mask = tfio.audio.freq_mask(dbscale_mel_spectrogram, param=10)\n",
    "# plt.figure()\n",
    "# plt.imshow(freq_mask.numpy())\n",
    "#\n",
    "# # Time masking\n",
    "# time_mask = tfio.audio.time_mask(freq_mask, param=6)\n",
    "# plt.figure()\n",
    "# plt.imshow(time_mask.numpy())\n",
    "#\n",
    "# # MFCC\n",
    "# mfccs = tf.signal.mfccs_from_log_mel_spectrograms(time_mask)\n",
    "# print(f'MFCC Shape: {mfccs.shape}')\n",
    "# print(mfccs)\n",
    "#\n",
    "# print(spectrogram_1)\n",
    "# spectrogram_1 = tf.expand_dims(spectrogram_1, -1)\n",
    "# spectrogram_1 = tf.image.resize_with_pad(spectrogram_1, 224, 224, method=ResizeMethod.LANCZOS5)\n",
    "# spectrogram_2 = tf.expand_dims(spectrogram_2, -1)\n",
    "# spectrogram_2 = tf.image.resize_with_pad(spectrogram_2, 224, 224, method=ResizeMethod.LANCZOS5)\n",
    "# spectrogram_3 = tf.expand_dims(spectrogram_3, -1)\n",
    "# spectrogram_3 = tf.image.resize_with_pad(spectrogram_3, 224, 224, method=ResizeMethod.LANCZOS5)\n",
    "# log_mel_spectrogram = tf.squeeze(tf.stack([spectrogram_1, spectrogram_2, spectrogram_3], axis=-1))\n",
    "# print(log_mel_spectrogram)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nDG46e46C5xH"
   },
   "source": [
    "stft_params = [{'nfft': 512, 'window': 501, 'stride': 21}, {'nfft': 1024, 'window': 1001, 'stride': 31}, {'nfft': 2048, 'window': 1600, 'stride': 61}]  # stride = hop size\n",
    "\n",
    "def train_preprocess(file_path):\n",
    "    # Read file into AudioIOTensor, need to specify dtype as running it inside a tf.data function without eager execution CHECKED\n",
    "    audio_tensor = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file(file_path), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "\n",
    "    # Trim noise (TODO: try with different gate, default 0.1)\n",
    "    # position = tfio.audio.trim(audio_tensor, axis=0, epsilon=0.1)  # epsilon = noise gate (how loud between 0 to 1)\n",
    "    # audio_tensor = audio_tensor[position[0]:position[1]]\n",
    "    # faded_audio_tensor = tfio.audio.fade(trimmed_audio_tensor, fade_in=1000, fade_out=1000, mode=\"logarithmic\")\n",
    "\n",
    "    # zero pad to 16000  CHECKED\n",
    "    zero_padding = tf.zeros([sample_rate] - tf.shape(audio_tensor), dtype=tf.float32)\n",
    "    audio_tensor = tf.cast(audio_tensor, tf.float32)\n",
    "    padded_audio_tensor = tf.concat([audio_tensor, zero_padding], 0)\n",
    "\n",
    "    # spectrogram CHECKED\n",
    "    spectrogram = tfio.audio.spectrogram(padded_audio_tensor, nfft=1024, window=1001, stride=31)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    spectrogram = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram1 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram2 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[1]['nfft'], window=stft_params[1]['window'], stride=stft_params[1]['stride']))\n",
    "    spectrogram3 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[2]['nfft'], window=stft_params[2]['window'], stride=stft_params[2]['stride']))\n",
    "\n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram1 = tfio.audio.melscale(spectrogram1, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram2 = tfio.audio.melscale(spectrogram2, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram3 = tfio.audio.melscale(spectrogram3, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    #\n",
    "    # Convert to db scale mel-spectrogram (TODO: to try)\n",
    "    # dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "    #\n",
    "    # Convert to log-mel spectrogram\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "    log_mel_spectrogram1 = tf.math.log(mel_spectrogram1 + 1e-6)\n",
    "    log_mel_spectrogram2 = tf.math.log(mel_spectrogram2 + 1e-6)\n",
    "    log_mel_spectrogram3 = tf.math.log(mel_spectrogram3 + 1e-6)\n",
    "    #\n",
    "    # Freq masking\n",
    "    freq_mask = tfio.audio.freq_mask(log_mel_spectrogram, param=10)\n",
    "    freq_mask1 = tfio.audio.freq_mask(log_mel_spectrogram1, param=10)\n",
    "    freq_mask2 = tfio.audio.freq_mask(log_mel_spectrogram2, param=10)\n",
    "    freq_mask3 = tfio.audio.freq_mask(log_mel_spectrogram3, param=10)\n",
    "\n",
    "    # Time masking\n",
    "    time_mask = tfio.audio.time_mask(freq_mask, param=6)\n",
    "    time_mask1 = tfio.audio.time_mask(freq_mask1, param=6)\n",
    "    time_mask2 = tfio.audio.time_mask(freq_mask2, param=6)\n",
    "    time_mask3 = tfio.audio.time_mask(freq_mask3, param=6)\n",
    "    #\n",
    "    # # MFCC\n",
    "    # mfccs = tf.signal.mfccs_from_log_mel_spectrograms(time_mask)\n",
    "\n",
    "    # label id extraction\n",
    "    label = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "    label_id = tf.argmax(label == labels)\n",
    "\n",
    "    # time_mask = tf.expand_dims(time_mask, -1)\n",
    "    # time_mask = tf.squeeze(tf.stack([time_mask, time_mask, time_mask], axis=-1))  # 3 channel\n",
    "\n",
    "    time_mask1 = tf.expand_dims(time_mask1, -1)\n",
    "    time_mask1 = tf.image.resize(time_mask1, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    time_mask2 = tf.expand_dims(time_mask2, -1)\n",
    "    time_mask2 = tf.image.resize(time_mask2, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    time_mask3 = tf.expand_dims(time_mask3, -1)\n",
    "    time_mask3 = tf.image.resize(time_mask3, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram = tf.squeeze(tf.stack([time_mask1, time_mask2, time_mask3], axis=-1))\n",
    "    log_mel_spectrogram.set_shape([224, 224, 3])\n",
    "    return log_mel_spectrogram, label_id  # Switch output here, log, dbscale, freq mask, time mask, mfccs\n",
    "\n",
    "def val_preprocess(file_path):\n",
    "    # Read file into AudioIOTensor, need to specify dtype as running it inside a tf.data function without eager execution CHECKED\n",
    "    audio_tensor = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file(file_path), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "\n",
    "    # Trim noise (TODO: try with different gate, default 0.1)\n",
    "    # position = tfio.audio.trim(audio_tensor, axis=0, epsilon=0.1)  # epsilon = noise gate (how loud between 0 to 1)\n",
    "    # audio_tensor = audio_tensor[position[0]:position[1]]\n",
    "    # faded_audio_tensor = tfio.audio.fade(trimmed_audio_tensor, fade_in=1000, fade_out=1000, mode=\"logarithmic\")\n",
    "\n",
    "    # zero pad to 16000  CHECKED\n",
    "    zero_padding = tf.zeros([sample_rate] - tf.shape(audio_tensor), dtype=tf.float32)\n",
    "    audio_tensor = tf.cast(audio_tensor, tf.float32)\n",
    "    padded_audio_tensor = tf.concat([audio_tensor, zero_padding], 0)\n",
    "\n",
    "     # spectrogram CHECKED\n",
    "    spectrogram = tfio.audio.spectrogram(padded_audio_tensor, nfft=1024, window=1001, stride=31)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    spectrogram = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram1 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram2 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[1]['nfft'], window=stft_params[1]['window'], stride=stft_params[1]['stride']))\n",
    "    spectrogram3 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[2]['nfft'], window=stft_params[2]['window'], stride=stft_params[2]['stride']))\n",
    "\n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram1 = tfio.audio.melscale(spectrogram1, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram2 = tfio.audio.melscale(spectrogram2, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram3 = tfio.audio.melscale(spectrogram3, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    #\n",
    "    # Convert to db scale mel-spectrogram (TODO: to try)\n",
    "    # dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "    #\n",
    "    # Convert to log-mel spectrogram\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "    log_mel_spectrogram1 = tf.math.log(mel_spectrogram1 + 1e-6)\n",
    "    log_mel_spectrogram2 = tf.math.log(mel_spectrogram2 + 1e-6)\n",
    "    log_mel_spectrogram3 = tf.math.log(mel_spectrogram3 + 1e-6)\n",
    "\n",
    "    # label id extraction\n",
    "    label = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "    label_id = tf.argmax(label == labels)\n",
    "\n",
    "    # log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n",
    "    # log_mel_spectrogram = tf.squeeze(tf.stack([log_mel_spectrogram, log_mel_spectrogram, log_mel_spectrogram], axis=-1))  # 3 channel\n",
    "    # log_mel_spectrogram.set_shape([517, 128, 3])\n",
    "    log_mel_spectrogram1 = tf.expand_dims(log_mel_spectrogram1, -1)\n",
    "    log_mel_spectrogram1 = tf.image.resize(log_mel_spectrogram1, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram2 = tf.expand_dims(log_mel_spectrogram2, -1)\n",
    "    log_mel_spectrogram2 = tf.image.resize(log_mel_spectrogram2, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram3 = tf.expand_dims(log_mel_spectrogram3, -1)\n",
    "    log_mel_spectrogram3 = tf.image.resize(log_mel_spectrogram3, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram = tf.squeeze(tf.stack([log_mel_spectrogram1, log_mel_spectrogram2, log_mel_spectrogram3], axis=-1))\n",
    "    log_mel_spectrogram.set_shape([224, 224, 3])\n",
    "    return log_mel_spectrogram, label_id  # switch output here to db or log\n",
    "\n",
    "def test_preprocess(file_path):\n",
    "    # Read file into AudioIOTensor, need to specify dtype as running it inside a tf.data function without eager execution CHECKED\n",
    "    audio_tensor = tf.squeeze(tf.cast(tfio.audio.decode_wav(tf.io.read_file(file_path), dtype=tf.int16), tf.float32) / 32768.0, axis=-1)\n",
    "\n",
    "    # Trim noise (TODO: try with different gate, default 0.1)\n",
    "    # position = tfio.audio.trim(audio_tensor, axis=0, epsilon=0.1)  # epsilon = noise gate (how loud between 0 to 1)\n",
    "    # audio_tensor = audio_tensor[position[0]:position[1]]\n",
    "    # faded_audio_tensor = tfio.audio.fade(trimmed_audio_tensor, fade_in=1000, fade_out=1000, mode=\"logarithmic\")\n",
    "\n",
    "    # zero pad to 16000  CHECKED\n",
    "    zero_padding = tf.zeros([sample_rate] - tf.shape(audio_tensor), dtype=tf.float32)\n",
    "    audio_tensor = tf.cast(audio_tensor, tf.float32)\n",
    "    padded_audio_tensor = tf.concat([audio_tensor, zero_padding], 0)\n",
    "\n",
    "     # spectrogram CHECKED\n",
    "    spectrogram = tfio.audio.spectrogram(padded_audio_tensor, nfft=1024, window=1001, stride=31)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    spectrogram = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram1 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[0]['nfft'], window=stft_params[0]['window'], stride=stft_params[0]['stride']))\n",
    "    spectrogram2 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[1]['nfft'], window=stft_params[1]['window'], stride=stft_params[1]['stride']))\n",
    "    spectrogram3 = tf.abs(tfio.audio.spectrogram(padded_audio_tensor, nfft=stft_params[2]['nfft'], window=stft_params[2]['window'], stride=stft_params[2]['stride']))\n",
    "\n",
    "    # Convert to mel-spectrogram\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram1 = tfio.audio.melscale(spectrogram1, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram2 = tfio.audio.melscale(spectrogram2, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    mel_spectrogram3 = tfio.audio.melscale(spectrogram3, rate=sample_rate, mels=80, fmin=20, fmax=8000)\n",
    "    #\n",
    "    # Convert to db scale mel-spectrogram (TODO: to try)\n",
    "    # dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "    #\n",
    "    # Convert to log-mel spectrogram\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "    log_mel_spectrogram1 = tf.math.log(mel_spectrogram1 + 1e-6)\n",
    "    log_mel_spectrogram2 = tf.math.log(mel_spectrogram2 + 1e-6)\n",
    "    log_mel_spectrogram3 = tf.math.log(mel_spectrogram3 + 1e-6)\n",
    "\n",
    "    # log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n",
    "    # log_mel_spectrogram = tf.squeeze(tf.stack([log_mel_spectrogram, log_mel_spectrogram, log_mel_spectrogram], axis=-1))  # 3 channel\n",
    "    # log_mel_spectrogram.set_shape([517, 128, 3])\n",
    "    log_mel_spectrogram1 = tf.expand_dims(log_mel_spectrogram1, -1)\n",
    "    log_mel_spectrogram1 = tf.image.resize(log_mel_spectrogram1, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram2 = tf.expand_dims(log_mel_spectrogram2, -1)\n",
    "    log_mel_spectrogram2 = tf.image.resize(log_mel_spectrogram2, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram3 = tf.expand_dims(log_mel_spectrogram3, -1)\n",
    "    log_mel_spectrogram3 = tf.image.resize(log_mel_spectrogram3, (224, 224), method=ResizeMethod.LANCZOS5)\n",
    "    log_mel_spectrogram = tf.squeeze(tf.stack([log_mel_spectrogram1, log_mel_spectrogram2, log_mel_spectrogram3], axis=-1))\n",
    "    log_mel_spectrogram.set_shape([224, 224, 3])\n",
    "    return log_mel_spectrogram  # switch output here to db or log\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_files).map(train_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val_files).map(val_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_files).map(test_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "for spectrogram, label in train_ds.take(1):\n",
    "    input_shape = spectrogram.shape\n",
    "# audio_tensor, label = get_waveform_and_label('s1_release/s1_release/bird/train_bird_8400.wav')\n",
    "# tensor = tf.cast(audio_tensor, tf.float32) / 32768.0\n",
    "# plt.figure()\n",
    "# plt.plot(tensor.numpy())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6jP8B9qC5xK",
    "outputId": "86983186-f53e-4830-b791-8ed4d71974e9"
   },
   "source": [
    "# Training\n",
    "batch_size = 16\n",
    "train_ds = train_ds.batch(batch_size).cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "print('Input shape:', input_shape)\n",
    "norm_layer = Normalization()\n",
    "norm_layer.adapt(train_ds.map(lambda x, _: x))\n",
    "\n",
    "# DenseNet201 pretrained\n",
    "xInput = Input(input_shape, dtype=tf.uint8)\n",
    "# x = Resizing(224, 224)(xInput)\n",
    "x = norm_layer(xInput)\n",
    "x = tf.cast(x, tf.float32)\n",
    "densenet201 = keras.applications.DenseNet201(include_top=False, weights='imagenet')\n",
    "densenet201.trainable = True\n",
    "x = densenet201(x)\n",
    "x = Bidirectional(LSTM(256))(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization(epsilon=1.001e-5)(x)\n",
    "x = Activation('relu')(x)\n",
    "xOutput = Dense(num_labels)(x)  # no activation as loss using logit=True\n",
    "model = tf.keras.models.Model(xInput, xOutput)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Do6njrBC5xL",
    "outputId": "86ee521b-393f-4469-a55c-d0bf5adbdbc2"
   },
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-6)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "epoch = 100\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1,\n",
    "                                     mode='auto', baseline=None, restore_best_weights=True),\n",
    "    # tf.keras.callbacks.ModelCheckpoint('./best_model',monitor='val_accuracy',save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1)\n",
    "]\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, to_file='SC1v2.png') # SC1_ensemble.png\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lLxKoOVC5xM",
    "outputId": "2dd00789-c470-4ae5-cc3a-36a55fe56e83"
   },
   "source": [
    "history = model.fit(train_ds, epochs=epoch, validation_data=val_ds, callbacks=callbacks, use_multiprocessing=True, verbose=1)\n",
    "model.save('sc1v2')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "njzgPFAcC5xM"
   },
   "source": [
    "# Generate prediction csv\n",
    "# print('Loading model...')\n",
    "# model = models.load_model('sc1v2')\n",
    "# print('Model loaded. Predicting...')\n",
    "i = 0\n",
    "predicted_labels, filenames = [], []\n",
    "for spectrogram in test_ds.batch(1):\n",
    "    filenames.append(os.path.basename(test_files[i].replace('_normalized', '')))\n",
    "    prediction = model(spectrogram)\n",
    "    prediction_value = tf.nn.softmax(prediction[0]).numpy()\n",
    "    predicted_label = labels[np.argmax(prediction_value)]\n",
    "    predicted_labels.append(predicted_label)\n",
    "    i+=1\n",
    "\n",
    "df = pd.DataFrame(list(zip(filenames, predicted_labels)))\n",
    "df.to_csv('challenge_2_team_Tensor is not flowing.csv', index=False, header=False)  # tested submission file format passed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kp_BNCIpC5xN"
   },
   "source": [
    "'''\n",
    "DenseNet201 with norm no preprocess, dropout 0.2: 91.38\n",
    "stft_params = [{'nfft': 512, 'window': 501, 'stride': 21}, {'nfft': 1024, 'window': 1001, 'stride': 31}, {'nfft': 2048, 'window': 1600, 'stride': 61}]  # stride = hop size\n",
    "before add data: 9291\n",
    "\n",
    "time and freq mask dont work well\n",
    "'''"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}